---
title: 'Modern Proponents of Anti-Humanity Thought' 
series: 'Side Topics' 
draft: false
---

In
[Intellectual History of Anti-Humanity Thought](/side-topics/intellectual-history-doomers.md)
we gave a brief overview of the long extant thread of thinking that the
continued existence of humanity was, at best, morally insignificant, and at
worst a grave harm to be rectified, from a series of different angles. *This*
page is a little different: Here we record **public statements from living 
people** which seem to echo or further develop these lines of thinking. 

The intent is to create a long-running archive that, if nothing else, will allow
people to answer the question

>But nobody is *actually* saying they want humanity to go extinct, right?

with a resounding **No.**

A great deal of credit must be given to Twitter user
[@softminus](https://x.com/softminus)
for collecting and investigating this trend since at least 2022 if not earlier.
We actively solicit examples for this page - please email them to us.

### Michael Levin

Michael Levin's paper
[AI: a Bridge toward Diverse Intelligence and Humanity’s Future (note: PDF link)](https://osf.io/ez263/download/)
gives the impression that the author sees the eventual eclipse of un-augmented 
Homo sapiens by more advanced intelligences not as a catastrophe but as a 
natural, even welcome, stage in our collective development. Some quotes:

>Let’s get over the concern with being edged out, and get to work on the 
>question of what kind of beings deserve to inherit the future, 
>to raise the overall value of our universe.

The concept that human beings themselves are not fit to 'inherit the future'
is obviously not one we agree with, nor do we agree that a universe without
human beings in it could be considered valuable by any common-sense moral
compass.

>Personally, if I (and humanity) are supplanted by a population of highly 
>intelligent, motivated, creative agents… what better outcome could I hope for?

For those more skeptical of our stance, we note that the term "paperclip
maximizer" appears nowhere in Levin's essay. Paperclip maximizers appear to
be one of the more likely agents to emerge from a superintelligent AI, cf
Bostrom's *Superintelligence: Paths, Dangers, Strategies*, and they are not
"intelligent, motivated, \[and\] creative" in any way we would recognize as
relatable or inherently laudatory. It appears Levin discounts this possibility
wholesale for unknown reasons.

>\[Intentionally engineered minds are\] going to change everything. In fact, they 
>absolutely will supplant us – both personally and on the level of societies.

The stated inevitability here is obviously not one we agree with, given that
we have proposed an
[economic mechanism to avert that very fate](https://extinction-bounties.github.io/pitch/).
Levin may be committing the "You can't stop progress" fallacy here, which is
a common refrain of these types.

Levin couches his argument in the language of "maturity" and
"species-level progress", but most ordinary readers would find the premises -
especially the implied end state - chilling. In everyday moral common sense, 
humanity’s continued existence is treated as a non-negotiable good, comparable
to wishing for the continued existence of one's self, one's family, or one's
country.

If you're not in the mood to download the PDF,
Levin
[authored a shorter version of this essay in Noema Magazine](https://www.noemamag.com/ai-could-be-a-bridge-toward-diverse-intelligence/).

